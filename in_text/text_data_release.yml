title: >-
  Examining the influence of deep learning architecture on generalizability for predicting stream temperature in the Delaware River Basin

abstract: >-
  This data release and model archive provides all data, code, and modelling results used in Topp et al. (2022) to examine the
  influence of deep learning architecture on generalizability when predicting stream temperature
  in the Delaware River Basin (DRB). Briefly, we modeled stream temperature in the DRB using
  two spatially and temporally aware process guided deep learning models (a recurrent graph convolution network - RGCN,
  and a temporal convolution graph model - Graph WaveNet). The associated
  manuscript explores how the architectural differences between the two models influence how
  they learn spatial and temporal relationships, and how those learned relationships influence a model's ability
  to accurately predict stream temperature as domains shift towards out-of-bounds conditions. This data release and model
  archive contains three zipped folders for 1) Data Preparation, 2) Modelling Code, and 3) Model Predictions. Instructions for running
  data preparation code and modelling code can be found in the README.md files in 01_Data_Prep and 02_Model_Code respectively.
      
authors: ["Simon N. Topp", "Janet Barclay", "Jeremy Diaz", "Alexander Y. Sun","Xiaowei Jia", "Dan Lu","Jeffrey M. Sadler", "Alison Appling"]
pubdate: 2022 # replace with actual year, e.g., 2020
doi: https://doi.org/10.5066/P9HU7BLR # replace with actual DOI

build-environment:  >-
  Multiple computer systems were used to generate these data, including linux and OSX. The open source languages R and Python were used on all systems.
  Instructions for setting up the compute environments required as well as run instructions are included in the Readme files of the data prep and model code zip files.

# ----associated publication----
larger-cites:
  -
    authors: ["Simon N. Topp", "Janet Barclay", "Jeremy Diaz", "Alexander Y. Sun","Xiaowei Jia", "Dan Lu","Jeffrey M. Sadler", "Alison Appling"]
    title: >-
      Stream temperature prediction in a shifting environment: The influence of deep learning architecture
    form: publication
    pubdate: 202X
    link: https://doi.org/XXXXXXX

# ----supporting publications----    
cross-cites:
  -
    authors: ["Samantha K. Oliver", "Rasha A. Atshan", "William D. Watkins", "Hayley R. Corson-Dosch", "Jacob A. Zwart", "Jordan S. Read"]
    title: >-
      Data release: Predicting water temperature in the Delaware River Basin: U.S. Geological Survey data release
    pubdate: 2021
    form: publication
    link: https://doi.org/10.5066/P9GD8I7A
    
process-date: !expr format(Sys.time(),'%Y%m%d')
file-format: Three zipped folders

entities:
  -
    data-name: 01_Data_Prep.zip
    data-description: >-
      A zipped file containing code to retrieve and format data for modelling
    attributes: 
    -
      attr-label: environment.yaml
      attr-def: >-
        A yaml file for creating the necessary Python environment to run the workflow
      attr-defs: This data release # location where attributes are deinfed - can be "This data release" or a link to another data source page
      data-min: NA
      data-max: NA
      data-units: NA
    -
      attr-label: config.yml
      attr-def: >-
        A configuration file for specifying items to pull from ScienceBase as well as cutoff values for those items and a list of segments to exclude from data processing.
      attr-defs: This data release # location where attributes are deinfed - can be "This data release" or a link to another data source page
      data-min: NA
      data-max: NA
      data-units: NA
    -
      attr-label: Snakefile
      attr-def: >-
        A Snakemake workflow for pulling and formatting data
      attr-defs: This data release # location where attributes are deinfed - can be "This data release" or a link to another data source page
      data-min: NA
      data-max: NA
      data-units: NA
    -
      attr-label: makeArrays.py
      attr-def: >-
        Utility functions for formatting pulled csv files to zarr format for modelling
      attr-defs: This data release # location where attributes are deinfed - can be "This data release" or a link to another data source page
      data-min: NA
      data-max: NA
      data-units: NA
    -
      attr-label: makeDistanceMatrix.py
      attr-def: >-
        Utility functions for creating a distance matrix representing the Delaware River Basin river network
      attr-defs: This data release # location where attributes are deinfed - can be "This data release" or a link to another data source page
      data-min: NA
      data-max: NA
      data-units: NA
    -
      attr-label: getSciencebaseItems.py
      attr-def: >-
        Utility functions for pulling down items from ScienceBase
      attr-defs: This data release # location where attributes are deinfed - can be "This data release" or a link to another data source page
      data-min: NA
      data-max: NA
      data-units: NA
    -
      attr-label: ChristianaReaches.csv
      attr-def: >-
        A csv file with reach IDs to subset the Christiana basin as a small test basin for modelling efforts.
      attr-defs: This data release # location where attributes are deinfed - can be "This data release" or a link to another data source page
      data-min: NA
      data-max: NA
      data-units: NA
    -
      attr-label: data_out
      attr-def: >-
        An empty folder to hold the outputs of XX
      attr-defs: This data release # location where attributes are deinfed - can be "This data release" or a link to another data source page
      data-min: NA
      data-max: NA
      data-units: NA
    -
      attr-label: logs_out
      attr-def: >-
        An empty folder to hold the outputs of XX
      attr-defs: This data release # location where attributes are deinfed - can be "This data release" or a link to another data source page
      data-min: NA
      data-max: NA
      data-units: NA
    -
      attr-label: README.md
      attr-def: >-
        A markdown file detailing how to run the included workflow
      attr-defs: This data release # location where attributes are deinfed - can be "This data release" or a link to another data source page
      data-min: NA
      data-max: NA
      data-units: NA
  -
    data-name: 02_Model_Code.zip
    data-description: >-
      A zipped file of the modelling codebase
    attributes: 
    -
      attr-label: README.md
      attr-def: >-
        A markdown file describing how to run the provided workflows.
      attr-defs: This data release # location where attributes are deinfed - can be "This data release" or a link to another data source page
      data-min: NA
      data-max: NA
      data-units: NA
    -
      attr-label: river-dl
      attr-def: >-
        A folder containing the source code, including models, pre-processing steps, training routines, and model evaluation.
      attr-defs: This data release # location where attributes are deinfed - can be "This data release" or a link to another data source page
      data-min: NA
      data-max: NA
      data-units: NA
    -
      attr-label: environment.yaml
      attr-def: >-
        An environment yaml for creating a conda environment with the necessary libraries.
      attr-defs: This data release # location where attributes are deinfed - can be "This data release" or a link to another data source page
      data-min: NA
      data-max: NA
      data-units: NA
    -
      attr-label: setup.py
      attr-def: >-
        A short setup script to required to install river-dl as a library.
      attr-defs: This data release # location where attributes are deinfed - can be "This data release" or a link to another data source page
      data-min: NA
      data-max: NA
      data-units: NA
    -
      attr-label: workflow_examples
      attr-def: >-
        A directory with example Snakemake workflows and associated configuration files to conduct
        basic training and evaluation of models contained in the associated repository.
      attr-defs: This data release # location where attributes are deinfed - can be "This data release" or a link to another data source page
      data-min: NA
      data-max: NA
      data-units: NA
    -
      attr-label: setup.py
      attr-def: >-
        XX
      attr-defs: This data release # location where attributes are deinfed - can be "This data release" or a link to another data source page
      data-min: NA
      data-max: NA
      data-units: NA
  -
    data-name: 03_Model_Predictions.zip
    data-description: >-
     A zipped file of all modelling results and the Snakemake workflows that generated them.
     Results are organized into 6 folders. Four folders (baseline, drought, LLO, and LTO) contain model
     predictions and evaluation metrics broken down by reach, month, and overall performance. These files
     are provided for two models and for ten replicate runs per model. One folder (Snakemake_Workflows) contains
     the workflows for reproducing those results, and one folder (xai_outputs) contains the results from the explainable AI analysis.
    attributes: 
    -
      attr-label: Snakemake_Workflows # name of attribute 1
      attr-def: >-
        Snakemake workflows and associated configuration files used to generate results. Files in this
        folder include: Two .yml configuration files detailing model parameters for GWN and RGCN respectively, Snakefile_GWN.smk and Snakefile_RGCN.smk
        that contain workflows for training models across the 6 scenarios (results in baseline, Drought, LLO, and LTO folders), and three additional Snakefiles
        that contain workflows for the explainable XAI analysis contained here (xai_outputs folder).
      attr-defs: This data release. # location where attributes are deinfed - can be "This data release" or a link to another data source page
      data-min: NA
      data-max: NA
      data-units: NA
    -
      attr-label: LLO # name of attribute 2
      attr-def: >-
        Modelling results for leave-location-out experiments. These include experiments
        where models were trained on two of three geographic regions in the study area
        and tested on the third.  Folder includes training logs (finetune_log.csv), overall 
        performance metrics (overall_metrics.csv), monthly performance metrics (month_metrics.csv),
        and reach scale performance metrics (reach_metrics.csv). Please see included data dictionary
        for additional information on what each of these files contain.
      attr-defs: This data release. # location where attributes are deinfed - can be "This data release" or a link to another data source page
      data-min: NA
      data-max: NA
      data-units: NA
    -
      attr-label: LTO # name of attribute 2
      attr-def: >-
        Modelling results for the leave-time-out experiment where the ten warmest years
        are with withheld from training and used to test the models ability to generalize to 
        higher temperatures. Folder includes training logs (finetune_log.csv), overall 
        performance metrics (overall_metrics.csv), monthly performance metrics (month_metrics.csv),
        and reach scale performance metrics (reach_metrics.csv). Please see included data dictionary
        for additional information on what each of these files contain.
      attr-defs: This data release. # location where attributes are deinfed - can be "This data release" or a link to another data source page
      data-min: NA
      data-max: NA
      data-units: NA
    -
      attr-label: baseline # name of attribute 2
      attr-def: >-
        Modelling results for baseline models where the testing partition is well represented in the training partition. This folder additionally
        contains ten replicates of pre-training results where the models are trained on the outputs of the PRMS-SNTemp process model.
        Weights from these pre-training results are used to initialize models in all other training scenarios. Folder includes training logs 
        (pretrain_log.cvs, finetune_log.csv), overall performance metrics (overall_metrics.csv), monthly performance metrics (month_metrics.csv),
        and reach scale performance metrics (reach_metrics.csv). Please see included data dictionary
        for additional information on what each of these files contain.
      attr-defs: This data release. # location where attributes are deinfed - can be "This data release" or a link to another data source page
      data-min: NA
      data-max: NA
      data-units: NA
    -
      attr-label: Drought # name of attribute 2
      attr-def: >-
        Modelling results for the drought experiment where ten years with either officially declared
        droughts or the least precipitation withheld from training and used to test the models ability
        to generalize to higher temperatures.  Folder includes training logs (finetune_log.csv),
        overall performance metrics (overall_metrics.csv), monthly performance metrics (month_metrics.csv),
        and reach scale performance metrics (reach_metrics.csv). Please see included data dictionary
        for additional information on what each of these files contain
      attr-defs: This data release. # location where attributes are deinfed - can be "This data release" or a link to another data source page
      data-min: NA
      data-max: NA
      data-units: NA
    -
      attr-label: xai_outputs # name of attribute 2
      attr-def: >-
        Outputs from expected gradients and permutation based explainable AI (XAI). Please see
        data_dictionary_xai.xlsx for additional details on the contents of files in this folder.
      attr-defs: This data release. # location where attributes are deinfed - can be "This data release" or a link to another data source page
      data-min: NA
      data-max: NA
      data-units: NA

purpose: This release is relevant to various deep learning tasks, including model understanding and interrogation through explainable AI as well as implementation of complex deep learning architectures for water resource problems.
start-date: 19800401
end-date: 20191231

update: none planned
themekeywords: ["process guided deep learning", "generalization", "explainable AI (XAI)", "stream temperature", "domain shift", "machine learning", "hybrid modeling", "water","temperature","modeling","climate change"]

usage-rules: >-
  None

descgeog: "Zipped folders containing code and csv files detailing stream temperature predictions in the Delaware River Basin"
data-publisher: U.S. Geological Survey
indirect-spatial: Delaware River Basin
latitude-res: 0.1
longitude-res: 0.1

# ----contacts----
contact-person: Simon N. Topp
contact-phone: 303-917-2694
contact-email: stopp@usgs.gov
contact-position: Machine Learning Specialist
contact-address: "8505 Research Way"
contact-city: Middleton
contact-state: WI
contact-zip: 53562

metadata-person: Simon N. Topp
metadata-position: Machine Learning Specialist
metadata-phone: 303-917-2694
metadata-email: stopp@usgs.gov
metadata-address: "8505 Research Way"
metadata-city: Middleton
metadata-state: WI
metadata-zip: 53562
metadata-date: !expr format(Sys.time(),'%Y%m%d')

accur-test: No formal attribute accuracy tests were conducted.
funding-credits: >-
  This study was funded by the USGS with support from the U.S. Department of Energy, Office of Science, Office of Biological and Environmental
  Research, under Interagency Agreement Number 89243021SSC000068 as part of the ExaSheds project.
  This research used resources of the Core Science Analytics and Synthesis Advanced Research Computing program at the U.S. Geological Survey.

####New line and hyperlink examples
#<br> 
#  The data are stored in 4 child folders: 1) spatial information, 2) observations, and 3) driver data, 4) forecasts
#  <a href="url">link text</a>
process-description: >-
  Details of processing steps contained within this release are below, for additional information please
  see the relevant README.md within each zipped folder:
  
  
  (Processed in 01_Data Prep)
  
  1) Initial data is downloaded from ScienceBase (Oliver et al, 2021; https://doi.org/10.5066/P9GD8I7A). This data includes daily
  meteorological drivers (1980-2021), river reach characteristics, process model outputs of stream temperature and discharge, stream temperature observations,
  and an adjacency matrix detailing spatial relationships for 456 river reaches within the Delaware River Basin.
  All drivers, reach information, and observations are munged to remove missing values and extreme outliers.

  
  (Processed in 02_Model_Code)
  
  2) Code takes the output of (1) and separates it into training, validation, and test partitions as defined by the
  configuration files for the Snakemake workflows.
  
  
  3) Prepared data from (2) are used to train two different spatiotemporally aware process guided deep learning models.
  The first is a Recurrent Graph Convolution Network and the second is a Dilated Temporal Convolution Graph Model (GraphWave Net).
  Training is done in two phases for 6 different study scenarios (listed below). First, models are pre-trained on the process model
  outputs (PRMS-SNTemp) for the entire region and study period. After pre-training, models are fine-tuned on stream temperature
  observations restricted to the training period and reaches defined in the configuration file. Fine-tuning is done with an
  early stopping metric that stops training if the loss on the validation data partition hasn't improved in 20 epochs. This release
  contains results for six different training scenarios:
  

      - A baseline scenario where testing data is well represented in training data. Modelling results for this scenario are stored in 03_Model_Predictions\baseline.
      - A leave-time-out scenario where the warmest years are withheld from training to simulate performance under increasing temperatures. Modelling results for this scenario are stored in 03_Model_Predictions\LTO\max.
      - A leave-time-out scenario where the driest years are withheld from training to simulate performance under drought conditions. Modelling results for this scenario are stored in 03_Model_Predictions\Drought.
      - Three leave-location-out scenarios where different sets of biogeophysically distinct regions are withheld from training to simulate performance in unseen geographic domains. Modelling results for these scenarios are stored in 03_Model_Predictions\LLO\{location_name}.
  
  
  4) Trained models are used to make predictions on the hold-out test partitions. Error metrics, including bias, root mean squared error, and nash-sutcliffe efficiency
  are calculated for the overall partitions, by reach/partition, and by month/partition. These metrics can be found in 03_Model_Predictions in the
  "overall_metrics.csv", "reach_metrics.csv", and "month_metrics.csv" files for each model/training run respectively.
  
  
  5) Expected gradients and permutation based explainable AI (XAI) approaches are used to understand the learned spatial and temporal relationships
  within the models. These experiments examine how inputs at one location or timestep influence predictions at another. Workflows to recreate
  these experiments (Snakefile_permutation.smk, Snakefile_permutation_shuffle.smk, and Snakefile_expected_gradients.smk) can be found
  in 03_Model_Predictions/results/Snakemake_Workflows  and actual outputs can be found in 03_Model_Predictions/results/xai_outputs.
  
distro-person: Simon N. Topp
liability-statement: >-
  Unless otherwise stated, all data, metadata and related materials are considered to satisfy the quality standards relative to the purpose for which the data were collected.
  Although these data and associated metadata have been reviewed for accuracy and completeness and approved for release by the U.S. Geological Survey (USGS),
  no warranty expressed or implied is made regarding the display or utility of the data on any other system or for general or scientific purposes, nor shall
  the act of distribution constitute any such warranty.
